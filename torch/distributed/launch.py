from __future__ import absolute_import, division, print_function, unicode_literals
'\n`torch.distributed.launch` is a module that spawns up multiple distributed\ntraining processes on each of the training nodes.\n\nThe utility can be used for single-node distributed training, in which one or\nmore processes per node will be spawned. The utility can be used for either\nCPU training or GPU training. If the utility is used for GPU training,\neach distributed process will be operating on a single GPU. This can achieve\nwell-improved single-node training performance. It can also be used in\nmulti-node distributed training, by spawning up multiple processes on each node\nfor well-improved multi-node distributed training performance as well.\nThis will especially be benefitial for systems with multiple Infiniband\ninterfaces that have direct-GPU support, since all of them can be utilized for\naggregated communication bandwidth.\n\nIn both cases of single-node distributed training or multi-node distributed\ntraining, this utility will launch the given number of processes per node\n(``--nproc_per_node``). If used for GPU training, this number needs to be less\nor equal to the number of GPUs on the current system (``nproc_per_node``),\nand each process will be operating on a single GPU from *GPU 0 to\nGPU (nproc_per_node - 1)*.\n\n**How to use this module:**\n\n1. Single-Node multi-process distributed training\n\n::\n\n    >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n               YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other\n               arguments of your training script)\n\n2. Multi-Node multi-process distributed training: (e.g. two nodes)\n\n\nNode 1: *(IP: 192.168.1.1, and has a free port: 1234)*\n\n::\n\n    >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n               --nnodes=2 --node_rank=0 --master_addr="192.168.1.1"\n               --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n               and all other arguments of your training script)\n\nNode 2:\n\n::\n\n    >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n               --nnodes=2 --node_rank=1 --master_addr="192.168.1.1"\n               --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n               and all other arguments of your training script)\n\n3. To look up what optional arguments this module offers:\n\n::\n\n    >>> python -m torch.distributed.launch --help\n\n\n**Important Notices:**\n\n1. This utility and multi-process distributed (single-node or\nmulti-node) GPU training currently only achieves the best performance using\nthe NCCL distributed backend. Thus NCCL backend is the recommended backend to\nuse for GPU training.\n\n2. In your training program, you must parse the command-line argument:\n``--local_rank=LOCAL_PROCESS_RANK``, which will be provided by this module.\nIf your training program uses GPUs, you should ensure that your code only\nruns on the GPU device of LOCAL_PROCESS_RANK. This can be done by:\n\nParsing the local_rank argument\n\n::\n\n    >>> import argparse\n    >>> parser = argparse.ArgumentParser()\n    >>> parser.add_argument("--local_rank", type=int)\n    >>> args = parser.parse_args()\n\nSet your device to local rank using either\n\n::\n\n    >>> torch.cuda.set_device(arg.local_rank)  # before your code runs\n\nor\n\n::\n\n    >>> with torch.cuda.device(arg.local_rank):\n    >>>    # your code to run\n\n3. In your training program, you are supposed to call the following function\nat the beginning to start the distributed backend. You need to make sure that\nthe init_method uses ``env://``, which is the only supported ``init_method``\nby this module.\n\n::\n\n    torch.distributed.init_process_group(backend=\'YOUR BACKEND\',\n                                         init_method=\'env://\')\n\n4. In your training program, you can either use regular distributed functions\nor use :func:`torch.nn.parallel.DistributedDataParallel` module. If your\ntraining program uses GPUs for training and you would like to use\n:func:`torch.nn.parallel.DistributedDataParallel` module,\nhere is how to configure it.\n\n::\n\n    model = torch.nn.parallel.DistributedDataParallel(model,\n                                                      device_ids=[arg.local_rank],\n                                                      output_device=arg.local_rank)\n\nPlease ensure that ``device_ids`` argument is set to be the only GPU device id\nthat your code will be operating on. This is generally the local rank of the\nprocess. In other words, the ``device_ids`` needs to be ``[args.local_rank]``,\nand ``output_device`` needs to be ``args.local_rank`` in order to use this\nutility\n\n5. Another way to pass ``local_rank`` to the subprocesses via environment variable\n``LOCAL_RANK``. This behavior is enabled when you launch the script with\n``--use_env=True``. You must adjust the subprocess example above to replace\n``args.local_rank`` with ``os.environ[\'LOCAL_RANK\']``; the launcher\nwill not pass ``--local_rank`` when you specify this flag.\n\n.. warning::\n\n    ``local_rank`` is NOT globally unique: it is only unique per process\n    on a machine.  Thus, don\'t use it to decide if you should, e.g.,\n    write to a networked filesystem.  See\n    https://github.com/pytorch/pytorch/issues/12042 for an example of\n    how things can go wrong if you don\'t do this correctly.\n\n'
import sys
import subprocess
import os
from argparse import ArgumentParser, REMAINDER

def parse_args():
    """
    Helper function parsing the command line options
    @retval ArgumentParser
    """
    import custom_funtemplate
    return custom_funtemplate.rewrite_template('torch.distributed.launch.parse_args', 'parse_args()', {'ArgumentParser': ArgumentParser, 'REMAINDER': REMAINDER}, 1)

def main():
    import custom_funtemplate
    custom_funtemplate.rewrite_template('torch.distributed.launch.main', 'main()', {'parse_args': parse_args, 'os': os, 'sys': sys, 'subprocess': subprocess}, 0)
if __name__ == '__main__':
    main()

